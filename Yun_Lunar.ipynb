{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt7tuGXGMpWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "87c0ace0-a8be-4d00-dda5-e392389c41fc"
      },
      "source": [
        "!pip install gym\n",
        "!pip install box2d-py"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t5fG5GeMoSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5922dff-ea36-4037-e1f8-be9c6a147f79"
      },
      "source": [
        "# from  lunarLanding import DQNAgent\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Input, concatenate, add\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "print(K.tensorflow_backend._get_available_gpus())\n",
        "\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "import itertools\n",
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import random\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import os"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChNkl8k-MwNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d5cc2e2-b21a-4b46-e41e-e8118908823e"
      },
      "source": [
        "# from  lunarLanding import DQNAgent\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Input, concatenate, add\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "print(K.tensorflow_backend._get_available_gpus())\n",
        "\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "import itertools\n",
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import random\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import os"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vamVsQLiN_KG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from scipy.special import softmax\n",
        "import random\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "# Deep Q-learning Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, memsize = 7000, ga = 0.95, explore_rate = 1, explore_decay = 0.995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen = memsize)\n",
        "        self.gamma = ga    # discount rate\n",
        "        self.epsilon = explore_rate  # exploration rate\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = explore_decay\n",
        "        self.target_model = Sequential()\n",
        "        self.engine_model = Sequential()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        if not done:\n",
        "            self.memory.append((state, action, reward, next_state, done))\n",
        "        \n",
        "    def act(self, state):\n",
        "                \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            chose = np.random.randint(0,4)\n",
        "            return chose\n",
        "        \n",
        "        ACTION_SIZE = 4\n",
        "        STATE_SIZE = 8\n",
        "        bestStep = 0\n",
        "        initialStep = np_utils.to_categorical(0, ACTION_SIZE).reshape(1, ACTION_SIZE)\n",
        "        best_reward = self.target_model.predict([state.reshape(1, STATE_SIZE), initialStep])[0]#我有加[0]\n",
        "        for action in range(1, ACTION_SIZE):\n",
        "            step = np_utils.to_categorical(action, ACTION_SIZE).reshape(1, ACTION_SIZE)\n",
        "            predict = self.target_model.predict([state.reshape(1, STATE_SIZE), step])[0]#\n",
        "            if(predict > best_reward):\n",
        "                bestStep = action\n",
        "                best_reward = predict\n",
        "                \n",
        "        return bestStep\n",
        "#     def replay(self, batch_size):\n",
        "#         minibatch = random.sample(self.memory, batch_size)\n",
        "#         xs = []\n",
        "#         ys = []\n",
        "\n",
        "#         for state, action, reward, next_state, done in minibatch:\n",
        "#             target = reward\n",
        "\n",
        "#             if not done:\n",
        "#                 target = reward + np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
        "#             else:\n",
        "#                 target = np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
        "                \n",
        "#             xs.append(state[0])\n",
        "#             ys.append(target)\n",
        "#         xs = np.array(xs)\n",
        "#         ys = np.array(ys)\n",
        "#         self.model.fit(xs, ys, epochs= 1, verbose=0 , batch_size=batch_size)\n",
        "                \n",
        "#         if self.epsilon > self.epsilon_min:\n",
        "#             self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    def save_model(self, model_name = './checkpoint.h5', mem_name = 'memory'):\n",
        "        self.target_model.save(model_name)\n",
        "\n",
        "    def load_model(self,  model_name = './checkpoint-10.h5' , mem_name = 'memory-10.npy'):\n",
        "        self.target_model.load_weights(model_name)\n",
        "        self.engine_model.load_weights(model_name)\n",
        "        self.memory = np.load(mem_name, allow_pickle=True)\n",
        "        self.memory = deque(self.memory)\n",
        "    def learn (self):\n",
        "        ACTION_SIZE = 4\n",
        "        STATE_SIZE = 8\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon = self.epsilon_decay* 10\n",
        "        output = list(itertools.islice(agent.memory, 0, None))\n",
        "        xs_state = []\n",
        "        xs_action = []\n",
        "        ys = []\n",
        "        for i in output:\n",
        "            target = i[2]\n",
        "            if not i[4]:\n",
        "                Next_state = i[3].reshape(1, STATE_SIZE)\n",
        "                \n",
        "                bestStep = 0\n",
        "                initialStep = np_utils.to_categorical(0, ACTION_SIZE).reshape(1, ACTION_SIZE)\n",
        "                best_reward = self.target_model.predict([Next_state.reshape(1, STATE_SIZE), initialStep])[0]#我有加[0]\n",
        "                for action in range(1, ACTION_SIZE):\n",
        "                    step = np_utils.to_categorical(action, ACTION_SIZE).reshape(1, ACTION_SIZE)\n",
        "                    predict = self.target_model.predict([Next_state.reshape(1, STATE_SIZE), step])[0]#\n",
        "                    if(predict > best_reward):\n",
        "                        bestStep = action\n",
        "                        best_reward = predict\n",
        "                \n",
        "                Step = np_utils.to_categorical(bestStep, ACTION_SIZE).reshape(1, ACTION_SIZE)\n",
        "                \n",
        "                target += self.gamma * self.target_model.predict([i[3].reshape(1, STATE_SIZE), Step])[0]\n",
        "            ys.append(target)\n",
        "            \n",
        "#             print('reward',i[2])\n",
        "            step = np_utils.to_categorical(i[1], 4).reshape(1, 4)\n",
        "\n",
        "            xs_state.append(i[0][0])\n",
        "            xs_action.append(step)\n",
        "\n",
        "        ys = np.array(ys)\n",
        "        lgn = len(xs_state)\n",
        "        xs_action = np.array(xs_action).reshape(lgn, 4)\n",
        "            \n",
        "#         print(xs.shape)\n",
        "        self.engine_model.fit([xs_state, xs_action], ys, epochs = 4 , verbose = 0)\n",
        "        del self.memory\n",
        "        \n",
        "        self.memory = deque(maxlen = 9000)\n",
        "#         print(len(self.memory))\n",
        "        self.engine_model.save_weights(\"checkpoint-10.h5\")\n",
        "        self.target_model.set_weights(self.engine_model.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiC5MU_JMwkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "b46855fd-bc07-4e01-f545-d0defe450728"
      },
      "source": [
        "input_state = Input(shape=(8,))\n",
        "input_action = Input(shape=(4,))\n",
        "\n",
        "m = Dense(64)(input_state)\n",
        "m = BatchNormalization()(m)\n",
        "\n",
        "m = Dense(64, activation='relu')(m)\n",
        "m = BatchNormalization()(m)\n",
        "\n",
        "m = Dense(64, activation='relu')(m)\n",
        "m = BatchNormalization()(m)\n",
        "\n",
        "# m = Model(inputs = input_state, outputs = m)\n",
        "\n",
        "#####\n",
        "\n",
        "m_i = Dense(12,activation='relu')(input_action)\n",
        "m_i = BatchNormalization()(m_i)\n",
        "\n",
        "m_i = Dense(24,activation='relu')(m_i)\n",
        "m_i = BatchNormalization()(m_i)\n",
        "\n",
        "m_i = Dense(32,activation='relu')(m_i)\n",
        "m_i = BatchNormalization()(m_i)\n",
        "\n",
        "# m_i = Model(inputs = input_action, outputs = m_i)\n",
        "\n",
        "u = concatenate([m,m_i])\n",
        "\n",
        "u = Dense(64, activation='relu')(u)\n",
        "u = BatchNormalization()(u)\n",
        "\n",
        "u = Dense(32, activation='relu')(u)\n",
        "u = BatchNormalization()(u)\n",
        "\n",
        "u = Dense(1)(u)\n",
        "\n",
        "u = Model(inputs= [input_state,input_action ], outputs = u)\n",
        "\n",
        "u.compile(loss='mse', optimizer=Adam())\n",
        "u.summary()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           (None, 8)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_82 (Dense)                (None, 64)           576         input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_85 (Dense)                (None, 12)           60          input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 64)           256         dense_82[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 12)           48          dense_85[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_83 (Dense)                (None, 64)           4160        batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_86 (Dense)                (None, 24)           312         batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 64)           256         dense_83[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 24)           96          dense_86[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_84 (Dense)                (None, 64)           4160        batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_87 (Dense)                (None, 32)           800         batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 64)           256         dense_84[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 32)           128         dense_87[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 96)           0           batch_normalization_75[0][0]     \n",
            "                                                                 batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_88 (Dense)                (None, 64)           6208        concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 64)           256         dense_88[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_89 (Dense)                (None, 32)           2080        batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 32)           128         dense_89[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_90 (Dense)                (None, 1)            33          batch_normalization_80[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 19,813\n",
            "Trainable params: 19,101\n",
            "Non-trainable params: 712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCZIuPtSNrsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "state_size = 8\n",
        "action_size = 4\n",
        "agent = DQNAgent(state_size, action_size, memsize= 9000)\n",
        "\n",
        "agent.target_model = u\n",
        "agent.engine_model = u"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k6b0MCSMwyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0de7a6f0-0894-4ac9-dca9-dbc607043d31"
      },
      "source": [
        "done = False\n",
        "batch_size = 30\n",
        "game_history = [0]\n",
        "\n",
        "\n",
        "explore = True\n",
        "t_steps = 0\n",
        "for episode in range(3000):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "    start = time.time()\n",
        "    for timee in range(450):\n",
        "\n",
        "        action = agent.act(state)\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "        \n",
        "        total_reward += reward\n",
        "\n",
        "    game_history.append(total_reward)\n",
        "    if episode % 100 == 99:\n",
        "        print(\"epi:{} score: {} mean:{} spent:{}\".format(episode, total_reward, np.mean(game_history[-99:]), time.time() - start))\n",
        "        agent.learn()\n",
        "        agent.save_model()\n",
        "        np.save(\"game_history-10\", game_history)\n",
        "#         pl.figure(figsize=(40,20))\n",
        "#         pl.plot(game_history)\n",
        "#         display.clear_output(wait=True)\n",
        "#         display.display(pl.gcf())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epi:99 score: -134.65950092995024 mean:-85.17726316752419 spent:0.009893417358398438\n",
            "epi:199 score: 14.014314061262521 mean:-72.61469324357599 spent:0.010301351547241211\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}